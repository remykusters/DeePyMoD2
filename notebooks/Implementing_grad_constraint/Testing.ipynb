{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we test the gradient optimizer constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# DeepMoD stuff\n",
    "from deepymod_torch import DeepMoD\n",
    "from deepymod_torch.model.func_approx import NN\n",
    "from deepymod_torch.model.library import Library1D\n",
    "from deepymod_torch.model.constraint import LeastSquares, GradParams\n",
    "from deepymod_torch.model.sparse_estimators import Threshold\n",
    "from deepymod_torch.training import train_auto_split, train_auto_split_MSE, train_auto_split_test\n",
    "from deepymod_torch.training.sparsity_scheduler import TrainTest, Periodic, TrainTestPeriodic\n",
    "\n",
    "from phimal_utilities.data import Dataset\n",
    "from phimal_utilities.data.burgers import BurgersDelta\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device ='cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device = 'cpu'\n",
    "    \n",
    "# Settings for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "from deepymod_torch.utils.types import TensorList\n",
    "\n",
    "class GradConstraint(nn.Module):\n",
    "    def __init__(self, n_params) -> None:\n",
    "        super().__init__()\n",
    "        self.sparsity_masks: TensorList = None\n",
    "        self.coeff_vectors = torch.nn.ParameterList([torch.nn.Parameter(torch.rand(n_params, 1))])\n",
    "\n",
    "    def forward(self, input: Tuple[TensorList, TensorList]) -> Tuple[TensorList, TensorList]:\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            input (Tuple[TensorList, TensorList]): [description]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[TensorList, TensorList]: [description]\n",
    "        \"\"\"\n",
    "        time_derivs, thetas = input\n",
    "\n",
    "        if self.sparsity_masks is None:\n",
    "            self.sparsity_masks = [torch.ones(theta.shape[1], dtype=torch.bool).to(theta.device) for theta in thetas]\n",
    "        \n",
    "        #self.masked_coeff_vectors = [sparsity_mask[:, None] * coeff for sparsity_mask, coeff in zip(self.sparsity_masks, self.coeff_vectors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a dataset with many samples and low noise to be sure it works.\n",
    "v = 0.1\n",
    "A = 1.0\n",
    "\n",
    "x = np.linspace(-3, 4, 100)\n",
    "t = np.linspace(0.5, 5.0, 50)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "dataset = Dataset(BurgersDelta, v=v, A=A)\n",
    "    \n",
    "X, y = dataset.create_dataset(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1), n_samples=2000, noise=0.1, random=True, normalize=True)\n",
    "X, y = X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradParams(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       5000    100.00%               0s   1.96e-02   1.82e-02   1.36e-03   3.88e+01 "
     ]
    }
   ],
   "source": [
    "train_auto_split(model, X, y, optimizer, sparsity_scheduler, log_dir='data/initial_test/', write_iterations=25, max_iterations=5000, delta=0.005) # Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the probleem seems to be that the MSE doesn't move past pretty high... Let's try without noise and unnormalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a dataset with many samples and low noise to be sure it works.\n",
    "v = 0.1\n",
    "A = 1.0\n",
    "\n",
    "x = np.linspace(-3, 4, 100)\n",
    "t = np.linspace(0.5, 5.0, 50)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "dataset = Dataset(BurgersDelta, v=v, A=A)\n",
    "    \n",
    "X, y = dataset.create_dataset(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1), n_samples=2000, noise=0.0, random=True, normalize=False)\n",
    "X, y = X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradParams(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       3325     66.50%              64s   3.52e-02   2.48e-02   1.04e-02   6.14e+00 Algorithm converged. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "train_auto_split(model, X, y, optimizer, sparsity_scheduler, log_dir='data/test_no_noise_not_normalized/', write_iterations=25, max_iterations=5000, delta=0.005) # Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So again the MSE doesn't move past really high.... Why? Let's try writing a new base class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1821],\n",
       "        [-0.5422],\n",
       "        [ 0.2569],\n",
       "        [ 0.5052],\n",
       "        [ 0.4180],\n",
       "        [-0.3064],\n",
       "        [-0.8507],\n",
       "        [ 0.6535],\n",
       "        [ 0.2013],\n",
       "        [-0.1281],\n",
       "        [-0.2321],\n",
       "        [ 0.7307]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.constraint.sparsity_masks[0][:, None] * model.constraint.coeff_vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a new baseclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a dataset with many samples and low noise to be sure it works.\n",
    "v = 0.1\n",
    "A = 1.0\n",
    "\n",
    "x = np.linspace(-3, 4, 100)\n",
    "t = np.linspace(0.5, 5.0, 50)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "dataset = Dataset(BurgersDelta, v=v, A=A)\n",
    "    \n",
    "X, y = dataset.create_dataset(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1), n_samples=2000, noise=0.0, random=True, normalize=False)\n",
    "X, y = X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       5000    100.00%               0s   4.07e-02   3.19e-02   8.78e-03   2.42e+01 "
     ]
    }
   ],
   "source": [
    "train_auto_split(model, X, y, optimizer, sparsity_scheduler, log_dir='data/separate_base_class/', write_iterations=25, max_iterations=5000, delta=0.005) # Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WTF? The MSE still doesn't train? What if we train only the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a dataset with many samples and low noise to be sure it works.\n",
    "v = 0.1\n",
    "A = 1.0\n",
    "\n",
    "x = np.linspace(-3, 4, 100)\n",
    "t = np.linspace(0.5, 5.0, 50)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "dataset = Dataset(BurgersDelta, v=v, A=A)\n",
    "    \n",
    "X, y = dataset.create_dataset(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1), n_samples=2000, noise=0.0, random=True, normalize=False)\n",
    "X, y = X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       5000    100.00%               0s   5.04e-06   5.04e-06   1.35e+01   1.06e+02 "
     ]
    }
   ],
   "source": [
    "train_auto_split_MSE(model, X, y, optimizer, sparsity_scheduler, log_dir='data/separate_base_class_MSE/', write_iterations=25, max_iterations=5000, delta=0.005) # Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it trains the MSE; checking tensorboard shows that the estimator finds the right coefficients... However the coefficients update while they shouldn't, that's probably the cause of our problems. Let's adapt the MSE function until it doesn't update anymore. We start by not splitting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       1000    100.00%               0s   3.55e-05   3.55e-05   6.22e+00   5.24e+01 "
     ]
    }
   ],
   "source": [
    "train_auto_split_MSE(model, X, y, optimizer, sparsity_scheduler, log_dir='data/finding_bad_grads/', write_iterations=25, max_iterations=1000, delta=0.005) # Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't do the trick, it's still updating. What if we turn off the calculation of the regularisation term?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       1000    100.00%               0s   4.36e-05   4.36e-05   4.36e-05   6.55e+01 "
     ]
    }
   ],
   "source": [
    "train_auto_split_MSE(model, X, y, optimizer, sparsity_scheduler, log_dir='data/finding_bad_grads_2/', write_iterations=25, max_iterations=1000, delta=0.005) # Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still updates.... Weirdly the unscaled version doesn't update so it's probably something in the writing of the coeffs. What if we put all the writing in a no_grad block?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       1000    100.00%               0s   6.84e-05   6.84e-05   6.84e-05   1.02e+02 "
     ]
    }
   ],
   "source": [
    "train_auto_split_MSE(model, X, y, optimizer, sparsity_scheduler, log_dir='data/finding_bad_grads_3/', write_iterations=25, max_iterations=1000, delta=0.005) # Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, my guess is that it's somewhere in the coeff_vector() method which scales them...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       1000    100.00%               0s   3.86e-05   3.86e-05   3.86e-05   1.07e+01 "
     ]
    }
   ],
   "source": [
    "train_auto_split_MSE(model, X, y, optimizer, sparsity_scheduler, log_dir='data/finding_bad_grads_4/', write_iterations=25, max_iterations=1000, delta=0.000) # Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that did the trick, it was in that function... But fuck me, how much effect did this have the last few months...? Let's slowly build the function back up again and fix it...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No it was correct. The unscaled coeffs didnt update at all, just the scaled ones. So there's no bug, the problem is that the coefficients don't update..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figuring out why the coeffs don't update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a new training function based on auto_split to figure out why it doesn't update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the parameters from the optimizer shows that it's not added to the parameters, so apparently it's not registered in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraint = GradConstraint(12)\n",
    "[param for param in constraint.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we turn the list into a parameterlist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 1.6578],\n",
       "         [ 2.5803],\n",
       "         [ 0.9771],\n",
       "         [ 1.2829],\n",
       "         [ 0.1055],\n",
       "         [-0.8976],\n",
       "         [-0.4052],\n",
       "         [-0.1735],\n",
       "         [ 1.3589],\n",
       "         [ 0.1471],\n",
       "         [-0.2989],\n",
       "         [-0.0164]], requires_grad=True)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraint = GradConstraint(12)\n",
    "[param for param in constraint.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it works! Let's test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       1000    100.00%               0s   1.74e-02   1.71e-02   3.18e-04   2.05e+01 "
     ]
    }
   ],
   "source": [
    "train_auto_split(model, X, y, optimizer, sparsity_scheduler, log_dir='data/check_constraint_fixed/', write_iterations=25, max_iterations=1000, delta=0.005) # Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It updates! Now let's do a check by running it long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       5000    100.00%               0s   1.35e-02   1.25e-02   9.94e-04   1.47e+01 "
     ]
    }
   ],
   "source": [
    "train_auto_split(model, X, y, optimizer, sparsity_scheduler, log_dir='data/gradient_run_1/', write_iterations=25, max_iterations=5000, delta=0.00) # Runni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So very slow convergence... Let's switch the initialization to rand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       5000    100.00%               0s   7.20e-03   6.50e-03   6.98e-04   1.08e+01 "
     ]
    }
   ],
   "source": [
    "train_auto_split(model, X, y, optimizer, sparsity_scheduler, log_dir='data/gradient_run_2/', write_iterations=25, max_iterations=5000, delta=0.00) # Runni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That helped a bit I think, let's tweak the hyperparams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=25, patience=5000)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.99, 0.999), amsgrad=True, lr=2e-3) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "      10000    100.00%               0s   4.00e-06   2.00e-06   2.00e-06   2.26e+00 "
     ]
    }
   ],
   "source": [
    "train_auto_split(model, X, y, optimizer, sparsity_scheduler, log_dir='data/gradient_run_3/', write_iterations=25, max_iterations=10000, delta=0.00) # Runni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing nicely with deepmod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that works; now let's include the sparsity mask so that updates too. We make new training procedure cause a few things need to be changed there regarding the sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a dataset with many samples and low noise to be sure it works.\n",
    "v = 0.1\n",
    "A = 1.0\n",
    "\n",
    "x = np.linspace(-3, 4, 100)\n",
    "t = np.linspace(0.5, 5.0, 50)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "dataset = Dataset(BurgersDelta, v=v, A=A)\n",
    "    \n",
    "X, y = dataset.create_dataset(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1), n_samples=2000, noise=0.1, random=True, normalize=False)\n",
    "X, y = X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradConstraint(12) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = Periodic(initial_epoch=2000, periodicity=25)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.99, 0.999), amsgrad=True, lr=2e-3) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       7500     75.00%             104s   4.86e-04   4.30e-04   5.57e-05   1.93e+00 Algorithm converged. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "train_auto_split_test(model, X, y, optimizer, sparsity_scheduler, log_dir='data/implementing_1/', write_iterations=25, max_iterations=10000, delta=0.001, patience=100) # Runni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([False,  True,  True, False, False,  True, False,  True,  True, False,\n",
       "          True, False])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sparsity_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it works... It's a bit slow, but it'll do for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's adapt the constraint and other stuff to allow both gradient optimization and least squares..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradParams(12, 1) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = Periodic(initial_epoch=1000, periodicity=25)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.99, 0.999), amsgrad=True, lr=2e-3) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       2000    100.00%               0s   6.11e-03   5.31e-03   8.00e-04   1.00e+01 "
     ]
    }
   ],
   "source": [
    "train_auto_split_test(model, X, y, optimizer, sparsity_scheduler, log_dir='data/implementing_2/', write_iterations=25, max_iterations=2000, delta=0.001, patience=100) # Runni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "         False,  True])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sparsity_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so that works as well. Now let's check how to move things to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "device ='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a dataset with many samples and low noise to be sure it works.\n",
    "v = 0.1\n",
    "A = 1.0\n",
    "\n",
    "x = np.linspace(-3, 4, 100)\n",
    "t = np.linspace(0.5, 5.0, 50)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "dataset = Dataset(BurgersDelta, v=v, A=A)\n",
    "    \n",
    "X, y = dataset.create_dataset(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1), n_samples=2000, noise=0.1, random=True, normalize=False)\n",
    "X, y = X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradParams(12, 1) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = Periodic(initial_epoch=1000, periodicity=25)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.99, 0.999), amsgrad=True, lr=2e-3) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       2000    100.00%               0s   4.44e-03   3.83e-03   6.07e-04   1.96e+00 "
     ]
    }
   ],
   "source": [
    "train_auto_split_test(model, X, y, optimizer, sparsity_scheduler, log_dir='data/implementing_3/', write_iterations=25, max_iterations=2000, delta=0.001, patience=100) # Runni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU works as well! Now let's test with the train / test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradParams(12, 1) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(patience=200, periodicity=50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.99, 0.999), amsgrad=True, lr=2e-3) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "      10000    100.00%               0s   3.68e-04   3.64e-04   3.54e-06   3.54e+00 "
     ]
    }
   ],
   "source": [
    "train_auto_split_test(model, X, y, optimizer, sparsity_scheduler, log_dir='data/implementing_4/', write_iterations=25, max_iterations=10000, delta=0.001, patience=500) # Runni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the sparsity didn't get triggered... Let's add the regression cost as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradParams(12, 1) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(patience=200, periodicity=50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.99, 0.999), amsgrad=True, lr=2e-3) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "      10000    100.00%               0s   3.68e-04   3.65e-04   2.85e-06   2.40e+00 "
     ]
    }
   ],
   "source": [
    "train_auto_split_test(model, X, y, optimizer, sparsity_scheduler, log_dir='data/implementing_5/', write_iterations=25, max_iterations=10000, delta=0.001, patience=500) # Runni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this whole thing works, it just doesn't get triggered yet. Let's try it with a high noise one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a dataset with many samples and low noise to be sure it works.\n",
    "v = 0.1\n",
    "A = 1.0\n",
    "\n",
    "x = np.linspace(-3, 4, 100)\n",
    "t = np.linspace(0.5, 5.0, 50)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "dataset = Dataset(BurgersDelta, v=v, A=A)\n",
    "    \n",
    "X, y = dataset.create_dataset(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1), n_samples=1000, noise=0.4, random=True, normalize=False)\n",
    "X, y = X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = GradParams(12, 1) # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(patience=200, periodicity=50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.99, 0.999), amsgrad=True, lr=2e-3) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "      12000     48.00%             375s   5.67e-03   5.63e-03   4.40e-05   1.33e+00 Algorithm converged. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works too :-) Might require some tweaking, but we'll get there... Now, let's test least squares to make sure it stil; works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) #Clustering() # Sparse estimator \n",
    "constraint = LeastSquares() # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "  \n",
    "sparsity_scheduler = TrainTestPeriodic(patience=200, periodicity=50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.99, 0.999), amsgrad=True, lr=2e-3) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "      12975     51.90%             539s   5.64e-03   5.59e-03   4.92e-05   1.30e+00 Algorithm converged. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "train_auto_split_test(model, X, y, optimizer, sparsity_scheduler, log_dir='data/implementing_7/', write_iterations=25, max_iterations=25000, delta=0.001, patience=500) # Runni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(5, 5 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
